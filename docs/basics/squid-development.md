---
sidebar_position: 10
title: Development flow
description: Development flow for building squids
---

# Development flow

Below is a general outline of the squid development steps. 

### 0. Prerequisites

- Install [Squid CLI](/squid-cli/installation)
- Follow the [Quickstart](/quickstart) and scaffold a new squid project using [sqd init](/squid-cli/init) and a suitable template.

### 1. Model the data with a schema file

Start the development by defining [the schema](/basics/schema-file) of the target database in the `schema.graphql` in the squid root folder. The schema definition consists of regular GraphQL type declarations annotated with custom directives to define:
- relations between the target entities
- entity properties, property types and entity relations 
- indexes to be created in the target database
- the schema of the auto-generated GraphQL API

A full reference of the `schema.graphql` dialect is available in the [GraphQL API section](/basics/schema-file).

### 2. Generate TypeORM classes

The squid processor data handlers use [TypeORM](https://typeorm.io) entities
to interact with the target database during the data processing. All necessary entity classes are
generated by the squid framework from `schema.graphql` with 
```bash
npx squid-typeorm-codegen
```

By convention, the generated model classes are kept in `src/model/generated` by default. Custom user-defined entities can
be added in `src/model/index.ts`.

### 3. Generate the database migrations

For this step you need a clean Postgres database running locally:
```bash
# drop the old schema
make down
# start a clean postgres in Docker
make up
```

The database schema changes (including the initial DDL) are applied through migration files located at `db/migrations`. The migrations are generated by 
```bash
# remove the old migrations
rm -rf db/migrations/*.js
# build the sources
npm run build
# generated the new schema migrations in db/migrations
npx squid-typeorm-migration generate
```

Consult [database migrations](/basics/db-migrations) for more details.

### 4. Define the squid processor and the data handlers

A squid processor is a separate node.js process that fetches historical on-chain data, performs arbitrary transformations and saves the result into the target database schema defined above. By convention, the processor entry point is `src/processor.ts`.

- [`EvmBatchProcessor`](/evm-indexing) (imported from `@subsquid/evm-processor`) is used for EVM chains.
- [`SubstrateBatchProcessor`](/substrate-indexing) (imported from `@subsquid/substrate-processor`) is used for Substrate-based chains.



### 5. Initialize a suitable processor instance 
Configure the processor by defining:
- the archive endpoint
- indexing data ranges
- data to be extracted from Archives

**Example:**
```ts
const processor = new EvmBatchProcessor()
  .setDataSource({
    archive: 'https://eth.archive.subsquid.io',
  })
  .addTransaction([
    '0x0000000000000000000000000000000000000000'
  ], {
    range: {
      from: 6_000_000
    },
    data: {
      transaction: {
        from: true,
        input: true,
        to: true
      }
    }
  });
```

See [EvmBatchProcessor configuration](/evm-indexing/configuration) and [SubstrateBatchProcessor configuration](/substrate-indexing/configuration) for details.

### 6. Generate Typescript facade classes to decode the on-chain-data:
- For EVM data, use [`evm-typegen`](/basics/typegen/squid-evm-typegen) 
- For Substrate data, use [`substrate-typegen`](/basics/typegen/squid-evm-typegen)
- For Ink! smart contract data, use [`ink-typegen`](/basics/typegen/squid-wasm-typegen)

### 7. Define the processor batch handler defined by the `processor.run()` method. 
- For `EvmBatchProcessor`, see the [Data Mapping page](/evm-indexing/data-mapping) 
- For `SubstrateBatchProcessor`, see the [data handlers section](/substrate-indexing/data-handlers)

**Example:**
```ts
processor.run(new TypeormDatabase(), async (ctx) => {
  const entities: Map<string, FooEntity> = new Map();
  // process a batch of data 
  for (const c of ctx.blocks) {
    // the data is packed into blocks, 
    // each block contains only the requested items
    for (const irem of c.items) {
    if(e.kind === 'evmLog') {
        // decode, extract and transform the evm log data
        const { baz, bar, id } = extractLogData(e.evmLog)
        entities.set(id, new FooEntity({
            id,
            baz,
        bar
        })) 
    } 
    if (e.kind === 'transaction') {
        // decode tx data if requested
    }
  }
  // upsert data to the target db in single batch
  await ctx.store.save([...entities.values()])
});
```


For an end-to-end walkthrough, see

- [`EvmBatchProcessor` in action](/evm-indexing/batch-processor-in-action)
- [`SubstrateBatchProcessor` in action](/substrate-indexing/batch-processor-in-action)


### 8. Run the squid services

Run the processor with
```bash
make process
```

Run the GraphQL server in a separate terminal window with
```bash
make serve
```
The GraphQL playground is available at `http://localhost:4350/graphql`.

### 9. Deploy the squid

Follow the [Deploy Squid](/deploy-squid) section.

## What's next?

- Learn from the [Squids examples](/examples)
- Get familiar with the [typegen tools](/basics/typegen)